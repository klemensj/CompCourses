{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align =\"right\">Thomas Jefferson University <b>COMP 103</b>: Data Analysis and Visualization</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis and multivariate statistics\n",
    "\n",
    "The field of statistics that handles situations in which we have more than two variables to compare to one another is called **multivariate** statistics. We have already done the most basic multivariate analysis in the form of a two-way ANOVA, which includes one dependent variable and two independent variables. Extensions of this approach to  incude MANOVA (multivariate ANOVA) and multiple regression. \n",
    "\n",
    "So far, however, we have only looked at data in which we have no more that one 'response' or dependent variable at a time. But of course in the real world we will be often looking at situations in which the measurement of multiple dependent variables will be relevant to the same scientific question. \n",
    "\n",
    "In this notebook we will look at the **Principal Component Analysis**, or **PCA**. PCA is fundamentally an *exploratory* type of analysis. We use it to discover the underlying structure of variation in a dataset and then to reduce that variation down to something more manageable. All of the varaibles in a PCA are independent of one another, the goal is not to test the effect of an experimental treatment. \n",
    "\n",
    "## Getting Started - constructing a covariance matrix\n",
    "\n",
    "Take a look at the data below. We have created a situation in which 25 observations of variables a, b, and c are all related to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.random.normal(1, 0.1, 25)               # The parameters represent mean, SD, and number of samples\n",
    "b = 10*a + np.random.normal(10, 1, 25)         ## I have made these distributions with different means\n",
    "c = 100*a + np.random.normal(100, 10, 25)      ## but proportional SDs on purpose\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "\n",
    "ax1 = plt.subplot(1,3,1)\n",
    "plt.scatter(a,b)\n",
    "plt.title('x=a, y=b')\n",
    "\n",
    "ax1 = plt.subplot(1,3,2)\n",
    "plt.scatter(a,c)\n",
    "plt.title('x=a, y=c')\n",
    "\n",
    "ax1 = plt.subplot(1,3,3)\n",
    "plt.scatter(b,c)\n",
    "plt.title('x=b, y=c')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a, b, and c are all related to one another (we actually know why because we just manufactured this data, but in real life we wouldn't know the underlying relationship). We could run a regression line through each of these pairs of data points very easily! But that wouldn't tell us everything that we want to know, especially in a situation in which the relationship among the varialbes was more complicated, or if we had a larger number of variables to consider. \n",
    "\n",
    "An equivalent way to view this data is to consider the **covariance matrix** and the **correlation matrix** between these three variables using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov([a,b,c])\n",
    "cor_matrix = np.corrcoef([a,b,c])\n",
    "\n",
    "print('The covariance matrix of a, b, and c:\\n',cov_matrix)\n",
    "print()\n",
    "print('The correlation matrix of a, b, and c:\\n',cor_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. \n",
    "Why are these matrices different from one another? **Hint**, the first section of <a href = 'https://en.wikipedia.org/wiki/Covariance_and_correlation'>this Wikipedia article</a> may be a helpful reminder. \n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review activity\n",
    "A heatmap is a great way to visualize the covariance and correlation matrices for easy reading, make heatmaps that display those matrices side-by-side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    Your code for the heatmap here\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does PCA do?\n",
    "What PCA does, in a nutshell, is to apply an ordinary least squares regression problem in higher-dimensional space. We can visualize this in our simple example, as it is a three-dimensional problem (although at higher dimensionalities it won't be possible to visualize it). \n",
    "\n",
    "First we have to convince ourselves as there is a line that represents the 'least squares' fit for a regression analysis there must also be a line that reduces the squared distance to the line for all points in a 3D representation of the dataset. Take a look at the figure below, although it's not the greatest representation is it obvious that there will be a single line that mimimizes the squared distance to all of the points in the dataset? Try to convince yourself of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3D = plt.figure()\n",
    "ax = fig3D.add_subplot(projection='3d')\n",
    "ax.scatter(a,b,c, marker = 'o')\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line that has that property is the **First Principal Component** of this dataset.\n",
    "\n",
    "Now let's think about what happens next. You can see that although that line will describe a lot of the variation among our highly correlated variables, we could also draw a line that is perpendicular to that line, and thus independent of or **orthogonal** to it. Once again there are an infinite number of lines that we could draw perpendicular to the first line in 3D space, so we want to pick the one that minimizes the squared difference of all of the points to that line. This line represents our **Second Principle Component**. \n",
    "\n",
    "In our example we can see that what we will have done is create a new set of perpendicular axes floating somewhere out there in our cloud of points. The **Third principal component** (in this example!) is therefore completely constrained. There is only a *single line* that is perpendicular to the first and second principle components. Because our data is three dimensional, this component will capture all the rest of the variance in the dataset.\n",
    "\n",
    "### More than 3 dimensions\n",
    "\n",
    "Once we get beyond three indepenedent variables in our data, we can non longer rely on a visual analogy as our brains aren't set up to think in higher dimensions, but the underlying principles are exactly the same. If there are N variables in our data then our scatterplot would have to be presented in N-dimensional space, within that space there will be a mathematical structure that is equivalent to a line that minimizes the squared distance of all points to itself. That structure will possess an infinite set of structures that are orthogonal to it, and so on until the final principal component defines the complete set of new N-dimensional axes. Mathematically these structures are discovered via linear algebra, and are known as **eigenvectors** of the data. Python will handle all of that for us, however. \n",
    "\n",
    "Even when we are in higher-dimensional space, however, as the principle components are orthogonal to one another by definition, the output of a PCA will give us our principle components as values that can be plotted in two dimensional space as we will see below\n",
    "\n",
    "\n",
    "## Running a PCA\n",
    "\n",
    "We will use `statsmodels` for our PCA analysis, another major library, `scikitlearn` can also be employed. \n",
    "\n",
    "Let's first look at the <a href ='https://www.statsmodels.org/dev/generated/statsmodels.multivariate.pca.PCA.html'>documentation for the PCA function</a>. \n",
    "\n",
    "There are a lot of parameters here. Let's take a look at the set of three that are set to TRUE by default and that  include standardize, demean, and normalize. Go back and look at your answer to the question about the difference between the covariance and correlation matrices. Correlation, it turns out, is independent of the magnitude of the variables used - it is unitless. Covariance, however, is not. From the perspective of a PCA, having a large magnitude variable (like our variable c) will cause most of the covariance to be associated with variable c, and the choice of the first principle component will be unduly influenced by those aspects of the covariance matrix that include c as an element. \n",
    "\n",
    "Collectively these values reset our means to zero so that we can see the relative influence of each factor. Practically speaking this means that we are working with the correlation and not the covariance matrix of the variables. \n",
    "\n",
    "Another important variable is ncomp, which tells the PCA how many principle components it should calculate. \n",
    "\n",
    "Running the model is very easy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.pca import PCA\n",
    "\n",
    "abc_data = pd.DataFrame({'a':a,'b':b,'c':c})  # if we just feed in lists a, b, and c it will flip vars and obs\n",
    "\n",
    "abc_PCA = PCA(abc_data)\n",
    "\n",
    "print(abc_PCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like our PCA ran ok, but it doesn't have a default display object, so we will have to go in and pull out the data we need to work with to think about interpreting it. I think the following three elements are the most useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCA loadings\\nvalues can be intepreted as correlations between variables and components')\n",
    "print(abc_PCA.loadings)\n",
    "print()\n",
    "print('Cumulative R^2 value (cumulative variation\\nexplained as each component is added)')\n",
    "print(abc_PCA.rsquare)\n",
    "print()\n",
    "print('Coefficients for the projection onto PCA axes (see below)')\n",
    "print(abc_PCA.coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better approach is to use some of the built in methods to describe the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax = abc_PCA.plot_scree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax = abc_PCA.plot_rsquare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures correspond to the first two data outputs above. The size of the eigenvalue for a particular vector is a measure of how much of the variation it explains. The classic output of a PCA is the graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.scatter(abc_PCA.factors['comp_0'],abc_PCA.factors['comp_1'])\n",
    "ax.set_xlabel('PC 1', size =24, loc = 'left')\n",
    "ax.set_ylabel('PC 2', size = 24, loc = 'bottom')\n",
    "\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 'projection' of our original data down onto the first two PCA axes. Looking back at the tables above we would interpret this this way:\n",
    "\n",
    "- This projection captures X of the data in our orginal dataset. \n",
    "- Principal component 1 captures the fact that most of the variation in the data is explained by positive / negative correlations among/ between\n",
    "- Principal component 2 explains much less of the data, and is explained by \n",
    "\n",
    "Sometimes this projection will reveal important patterns in the data. For example, if data were strongly clustered along the PCA, we might interpret the result as meaning there were distinct groupings of observations present in the data. As we were using randomly generated data, here ... not so much.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Write your conclusions. What values did you use to come to those conclusions?\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "Go back and generate a new random data set and rerun the analyses.\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "Which variables stayed the same and which ones changed when the data changed?\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "What does this tell you about how we should interpret Principle components that don't explain much of the variation? Take into considertation that that we know the 'right answer' regarding the relationships among a, b, and c based on how the data was actually generated?\n",
    "\n",
    "- \n",
    "- \n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Exercise\n",
    "\n",
    "Now let's look at a *much* more interesting dataset. `Liko_Na_Pilina.csv`, located in your `data` folder, contains a data set of plan species traits collected by Rebecca Ostertag, an ecologist working in Hawaii, and her colleagues. A description of the variables included in this dataset and the rationale behind why this data was collected can be found in <a href = 'https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2664.12413'> this short article</a>.\n",
    "\n",
    "For this dataset I would like you to generate\n",
    "\n",
    "- heatmaps for the covariance and correlation matrices between the traits\n",
    "- a report of important values generated by the PCA\n",
    "- the scree plot and box plots for analyzing the PCA\n",
    "- a projection plot showing the first two principal components\n",
    "- in the text cell below, include an interpretation of what each of the principal components means, as well as an overall interpretation of your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here \n",
    "### (you may want to make multiple code windows to output differnet graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your interpretations of the PCA go here\n",
    "\n",
    "- \n",
    "- \n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TJU logo image](images/TJU_logo_image.png \"TJU logo image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
