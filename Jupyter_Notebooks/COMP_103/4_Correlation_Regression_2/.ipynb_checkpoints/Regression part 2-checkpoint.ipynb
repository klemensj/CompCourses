{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align =\"right\">Thomas Jefferson University <b>COMP 103</b>: Data Analysis and Visualization</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Regression part II\n",
    "\n",
    "\n",
    "Correlation and regression analysis are fundamental to data science AND to statistical analysis. Many statistical tests on parametric data are simply extensions of linear regression. In this notebook we will dive a little deeper into correlation and regression analyses. \n",
    "\n",
    "## Correlation vs. Regression\n",
    "\n",
    "**Correlation** is a measure of association between two variables, while **regression** implies a model in which we predict one value based on the value of another. Regression therefore assumes an independent x value that is used to predict a dependent y value. One of the *assumptions* of regression analysis is that x is measured without error, or in other words that x values are 'true' while y variables contain some degree of measurement error. On top of this measurement error is any difference in the y variable that is caused by factors that are not included in our model. The distances of the observed points (represented by the scatterplot) from the model (represented by the regression line) are known as the **residuals** of the regression analysis. The regression line itself (defined by slope and intercept) is the line that minimizes the total squared distance of the observations from the line. This most typical type of regression is therefore known as a **least squares regression**. \n",
    "\n",
    "Let's take a look at a very simple dataset based on this model. In this data, let's assume that we are studying the performance of a particular physiological task `perf` and that we are hypothesizing that this performance is dependent on the concentration of a particular chemical `conc`. We will scale both from 1 to 10. \n",
    "\n",
    "Create the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "perf = np.zeros(10)\n",
    "conc = list(range(1,11))\n",
    "\n",
    "for i in range(10):\n",
    "    perf[i] = conc[i] + (np.random.normal(0, 1))\n",
    "\n",
    "print(conc[i])\n",
    "print('conc (x) = ', conc)\n",
    "print('perf (y) = ', perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run the regression analysis and plot it. Note the various values from the regression analysis that are contained within our linregress object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "results = stats.linregress(conc,perf)       # This creates the regression object\n",
    "print(results)\n",
    "\n",
    "plt.ylabel('Performance')\n",
    "\n",
    "plt.xlabel('Concentration')\n",
    "\n",
    "plt.scatter(conc,perf)\n",
    "\n",
    "xs = [min(conc), max(conc)]\n",
    "ys = [min(conc) * results.slope + results.intercept, max(conc) * results.slope + results.intercept]\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "for i in range(len(conc)):\n",
    "    xs = [conc[i],conc[i]]\n",
    "    ys = [perf[i], conc[i]*results.slope + results.intercept]\n",
    "    plt.plot(xs, ys, color = 'red')\n",
    "\n",
    "r2_text = 'R^2= ' + \"{:.3f}\".format(results.rvalue**2)\n",
    "eq_text = 'y = ' + \"{:.3f}\".format(results.slope) + 'x + ' + \"{:.3f}\".format(results.intercept)\n",
    "\n",
    "plt.text(7,3, r2_text)\n",
    "plt.text(7,2, eq_text)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "What did the following line of code do when we were defining the dataset (be specific):\n",
    "```\n",
    "for i in range(10):\n",
    "    perf[i] = conc[i] + np.random.normal(0, 1) \n",
    "```\n",
    "What do the red lines represent on the graph?\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you predict would happen if we modified the code above to:\n",
    "```\n",
    "for i in range(10):\n",
    "    perf[i] = conc[i] + np.random.normal(0, 5) \n",
    " ```\n",
    "What do you predict will happen? What do you observe if you make that change and rerun both code windows above? Be specific about the redlines and the R^2 value. \n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals \n",
    "\n",
    "The red lines in the diagram above are known as **residuals**. Residuals are defined as <a href='https://statdictionary.com/basic_stat_terms/r/residual/'>the difference between an observed value of a variable and its predicted value</a>. As we mentioned above regression is a *model* of how the world works.  In this model of the world, if we know the value of *x* we can predict the value of *y*. Residuals are therefore a measure of how accurate that prediction is, or in other terms, how much of the variation in the data remains unexplained by our model. A model with high R^2 values will have small residuals, and vice versa. R^2 can be thought of us as the percentage of the variance in our model that is explained by the relationship between *x* and *y*. \n",
    "\n",
    "We can extract the residuals from a regression with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.zeros(10)\n",
    "\n",
    "for i in range(10):\n",
    "    residuals[i] = perf[i]-(results.intercept + results.slope * conc[i])\n",
    "    \n",
    "plt.scatter(conc, residuals)\n",
    "plt.axhline(y=0, linestyle = 'dashed')\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('Concentration')\n",
    "\n",
    "print(residuals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a cheatcode version of the residuals plot in `seaborn`:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.residplot(x= conc, y = perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Stats and the Assumptions of Regression Models\n",
    "\n",
    "Regression models are at the root of many different kinds of statistical analysis. In this class we will focus on parametric statistics, in which we generally assume that the data we are analyzing is drawn from a normal distribution. \n",
    "\n",
    "As we move through the different statistical tests we are going to see that each one depends on certain assumptions. Specifically the assumptions of a simple linear regression are that:\n",
    "* *x and y have a linear relationship\n",
    "* x values are measured without error, all error is in the y variable\n",
    "* the y responses are indpendent of one another (specifically, the errors or residuals are uncorrelated). In practical terms, this would mean that the residuals would get larger as the x values get larger (the most common pattern) or the opposite would be true. If errors are normal, residuals should be evenly scattered around 0.\n",
    "* the data comes from a normal distribution (specifically, that the residuals are normally distributed). If the data are from a normal distribution, the errors will be uncorrelated as well, by definition. \n",
    "\n",
    "And there are two more assumptions that are relevant when there is more than one predictor variable. These aren't going to be very relevant to us yet but completeness they are:\n",
    "* If you have more than one predictor variables, they aren't correlated\n",
    "* *This is just an extension of the first assumption, but if there is more than one predictor variable, the y is a function of 'linear combinations' of the predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Run the code below to generate a dataset that compares 'calendar age' to a persons 'health age', then run a regression and create a scatter plot showing the R^2 value and a residuals plot for that data. Interpret the residuals plot. Does the data meet the assumptions of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to generate the dataset. Don't focus on interpreting this equation for outcome, focus on the data. \n",
    "\n",
    "outcome = np.zeros(250)\n",
    "age = np.random.randint(18,65,250)\n",
    "random = np.random.normal(0,5, 250)\n",
    "outcome = (age + random*(2*age/36))\n",
    "\n",
    "#\n",
    "## Your code here\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your interpretation of the results below:\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another *assumption* of regression analysis makes an assumption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "\n",
    "A new dataset for doing exploratory data analysis is the <a href = 'https://allisonhorst.github.io/palmerpenguins/articles/intro.html'>penguin dataset</a> based on the work of Kristin Gorman (Horst, Hill, and Gorman, 2020). Use body mass as an x variable assuming that it is an errorless measure of penguin size. I have added it to this module as `penguins.csv` in the data folder. Load this dataset into a pandas dataframe and then create the following:\n",
    "\n",
    "1. a 3-panel scatterplot showing the relationship of all three morphological variables to body size. Make sure that different species can be compared on your plots\n",
    "1. Pick the relationship between size and morphology that seems most different among penguin species. Use regression analyses to compare the relationships between morphology and size among species. Plot the relationships and indicate the R^2 value for each regression. \n",
    "1. For the relationship you have chosen test the assumptions of your regression analysis using a residuals plot. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TJU logo image](images/TJU_logo_image.png \"TJU logo image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source citation\n",
    "\n",
    "Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer\n",
    "Archipelago (Antarctica) penguin data. R package version 0.1.0.\n",
    "https://allisonhorst.github.io/palmerpenguins/. doi:\n",
    "10.5281/zenodo.3960218."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
